{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4eA+21UL2zrqQPVdvg8Yc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yingjun-mou/CLIP/blob/master/Reproduce_CLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kNXK9oei3GzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notework aims to reproduce CLIP model proposed in *Learning Transferable Visual Models From Natural Language Supervision*."
      ],
      "metadata": {
        "id": "Ja4hxWfm3Hpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1. Use exisiting open_clip model\n",
        "\n",
        "Task: Use open_clip to perform image classifications"
      ],
      "metadata": {
        "id": "4imkakaw4Tny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1. Colab preparation"
      ],
      "metadata": {
        "id": "sf3l13bI4isq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xavNBypd3A-m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install open_clip_torch matplotlib"
      ],
      "metadata": {
        "id": "SrVVLrY54qgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "IiKMg0ng5ES7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Loading the model"
      ],
      "metadata": {
        "id": "EcrGsBe55LiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import open_clip\n",
        "\n",
        "# List the names of all available CLIP models.\n",
        "open_clip.list_pretrained()"
      ],
      "metadata": {
        "id": "W2BSSKWz5itq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load one of the models.\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('convnext_base_w', pretrained='laion2b_s13b_b82k_augreg')"
      ],
      "metadata": {
        "id": "_yjE50MM53Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)\n"
      ],
      "metadata": {
        "id": "NSjJ8mZ76SYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Image Preprocessing"
      ],
      "metadata": {
        "id": "9solq7JC7FHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* normalize the pixel intensity using the dataset mean and standard deviation\n",
        "* resize the input images\n",
        "* center-crop them to conform with the image resolution that model expects"
      ],
      "metadata": {
        "id": "yPlpX9IA7P_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess"
      ],
      "metadata": {
        "id": "kKV3rBqp7Iza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4. Text Preprocessing"
      ],
      "metadata": {
        "id": "YC6UwPqP7Khi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Use a case-insensitive tokenizer `tokenizer.tokenize()`. It will pad the outputs to become 77 tokens long, which is the CLIP model expects."
      ],
      "metadata": {
        "id": "g6GDV-UG70wT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from open_clip import tokenizer"
      ],
      "metadata": {
        "id": "AF6aNIfw7ycS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(\"Hello World!\")"
      ],
      "metadata": {
        "id": "8HKdGlXG8MvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5. Set up input images and texts"
      ],
      "metadata": {
        "id": "fHUGx-luF6M0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to feed 8 example images and their textual descriptions to the model, and compare the similarity between the corresponding features.\n",
        "\n",
        "The tokenizer is case-insensitive, and we can freely give any suitable textual descriptions."
      ],
      "metadata": {
        "id": "4l4sfSLCGBoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# images in skimage to use and their textual descriptions\n",
        "descriptions = {\n",
        "    \"page\": \"a page of text about segmentation\",\n",
        "    \"chelsea\": \"a facial photo of a tabby cat\",\n",
        "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
        "    \"rocket\": \"a rocket standing on a launchpad\",\n",
        "    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n",
        "    \"camera\": \"a person looking at a camera on a tripod\",\n",
        "    \"horse\": \"a black-and-white silhouette of a horse\",\n",
        "    \"coffee\": \"a cup of coffee on a saucer\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "TntCeI_pF_uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_images = []\n",
        "images = []\n",
        "texts = []\n",
        "plt.figure(figsize=(16,5))\n",
        "\n",
        "for filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
        "  name = os.path.splitext(filename)[0]\n",
        "  if name not in descriptions:\n",
        "    continue\n",
        "\n",
        "  image = Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\")\n",
        "\n",
        "  plt.subplot(2, 4, len(images) +1)\n",
        "  plt.imshow(image)\n",
        "  plt.title(f\"{filename}\\n{descriptions[name]}\")\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  original_images.append(image)\n",
        "  images.append(preprocess(image))\n",
        "  texts.append(descriptions(name))\n",
        "\n",
        "plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "dgbGlu32Hnwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bExADS8V8Mcb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}